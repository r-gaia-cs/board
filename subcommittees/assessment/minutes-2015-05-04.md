2015-05-04

Attendees:
- Jason Williams
- Jeramia Ory
- Daniel Chen

<https://github.com/chendaniely/assessment-student-post-workshop/issues/1>


JW: E-board try to meet within first week of June

JW: We should seed with an introductory set of questions by end of May. 
Could be enough for a blog post.  These first set of questions would be
for technical questions for students (e.g., Can you re-run the 4th last
command).  Overall attitudinal changes.

On the Data Carpentry hackathon: how can we get informative questions during the lesson?  We can update every lesson (start with shell) with some type of
online 'quiz'. We can do peer scoring, instead of doing programmatic checks.

JO: By checking answers in real-time we can guarantee a certain rate of
feedback.

JW: then a final login for final assessment

JO: ipython notebook has a notebook grader that can be captured and
semi-automatically graded. <https://github.com/jupyter/nbgrader>

JO: Instructors have been shielded from the less 'fun' parts of
teaching, i.e., giving tests.  This is a risk and an area where
compliance may fall.

JW: We will be told when instructors are uncomfortable, and we can
adjust accordingly.  We can first present an ideal.

How to develop assessment questions for a module?

1.  Think about the learning objectives for the module:

-   What's the motivation for this lesson?
-   What mindset change should the learner have? (i.e., awareness of
    certain techniques)
-   What skills should the learner leave the room w/?
-   What is the knowledge they should work (walk?) away with. 
    

2.  Come up with questions that address the learning objectives 


-   There are three basic types of questions you should try to form
    -   **Attitude/Opinions/Mindsets** - These are potential
        opportunities to motivate or discourage your learners - you need
        to know these
    -   **Facts/Declarative Knowledge** - These are pieces of knowledge
        that your learner should walk away with; they may still not be
        able to use these facts, but this is a baseline of knowledge.
        Even if they stop learning here, these will be useful in future
        learning, collaborating, and just make our learners better
        people. improving your google fu
    -   **Actual, Testable, Skills**: These are the black-and-white,
        no-try-only-do skills that we should be able to test and see if
        learners have successfully acquired. Should be an application of
        either the recently learned facts or tying something together
        from earlier in the workshop.
        

Don't worry about sorting the questions in categories at first, the
assessment team will help with that if you aren't sure of if questions
fit into multiple categories. 

3. Submit your questions as pull lessons to the appropriate issues...if
possible include how you have categorized the question. If it is a
skills question indicate what lesson it applies to (novice-python,
shell, etc.). 


-   overall attitudes - these are very general and should apply to any
    learner at SWC workshop:
    -   <https://github.com/chendaniely/assessment-student-post-workshop/issues/2>
-   overall skills - these are very broad skills, that can be taken from
    any element of the core lessons, but should not be too language or
    platform specific. (e.g. know what an if statement is and how it
    works, but not addressing how shell if  syntax is different from 
    python if syntax:
    -   <https://github.com/chendaniely/assessment-student-post-workshop/issues/3>
-   overall declaratory knowledge - again, very general facts that you
    think are important for every SWC learner (novice to expert) to walk
    away with:
    -   <https://github.com/chendaniely/assessment-student-post-workshop/issues/4>
        

**Advice for writing questions**
Here is some advice and examples for writing good questions.  These are
really vague and generic so we can brainstorm.

1. Here are very basic examples for the question types:

-   **Attitude (I FEEL this about X) : **

    Learners should **feel** that using the shell can increase their
productivity 

-   **Declarative Knowledge (I KNOW this about X): **

    Learners know 2 resources (websites, books, etc.) where they can
extend their         knowledge  about the shell

-   **Skill (I can DO X):**

    Learner can write a 'for' loop that renames a collection of files
2. Where possible be measurable, specific, and, precise:

Action items:

-   Draft blog post and request for input from assessment list (5/8)
    -   Ask people see if we are measuring what we want (e.g. we need a
        list of hypotheses about SWC training) (5/8)
-   Ask larger SWC community to submit questions via pull requests
    (5/15)
-   In person-meeting and barn-raising first week in June
-   Ping Greg about previous grant submissions about things we missed?
    

Things to keep in mind:

-   Validity of assessment (during the assessment development do not
    refine it to the point were it's only 1 question per learning
    objective.  We need redundancy for reliability and validity)
-   do our questions match hypothesis
-   alpha/beta testing of questions
-   technology to use (survey monkey + some notebook grader)